{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYk8NG3yOIT9"
   },
   "source": [
    "### A MNIST-like fashion product database\n",
    "\n",
    "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doRRn93l0fZB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFO6PuxzOIT_"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efNjNImfOIUC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l9C4aAIGOIUH",
    "outputId": "6e9a078a-7100-49f7-a975-5d9d9510e0ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcoZBStrOIUQ"
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA1WsFSeOIUS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "qnbx7TyQOIUY",
    "outputId": "32447d12-c8b1-49a2-aafa-2c19b0a38207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 3us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UbiHj5YPOIUc",
    "outputId": "5a89aa4f-1625-427c-ed68-8c8c86fe8f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6]\n"
     ]
    }
   ],
   "source": [
    "print(testY[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "97k6f7x30yT6",
    "outputId": "1ce295fb-adf0-44ce-dfc4-9696d0d5f537"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o_eLrdPH02kp",
    "outputId": "288eecad-8b9d-44d3-d071-d1d2253959ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TLbaLWTq06yC",
    "outputId": "1df92579-4944-4677-8f41-1bdcc5c132b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "emI_Nuj40-_u",
    "outputId": "27c976df-0d2a-4a90-8c90-6ea6339452e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDAYzkwyOIUj"
   },
   "source": [
    "### Convert both training and testing labels into one-hot vectors.\n",
    "\n",
    "**Hint:** check **tf.keras.utils.to_categorical()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBlfYlANOIUk"
   },
   "outputs": [],
   "source": [
    "trainX = trainX.astype('float32')\n",
    "trainY = trainY.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNwsnZ1F1RvI"
   },
   "outputs": [],
   "source": [
    "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
    "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "RHV3b9mzOIUq",
    "outputId": "8ba05c5c-b1ab-4885-8be1-ef822eaa664f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "First 5 examples now are:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainY.shape)\n",
    "print('First 5 examples now are: ', trainY[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwhQ8e7VOIUw"
   },
   "source": [
    "### Visualize the data\n",
    "\n",
    "Plot first 10 images in the triaining set and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "AvDML2OoOIUx",
    "outputId": "62b17c54-fa2d-479c-94d5-a99b739494b3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAADjCAYAAABtjatfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL for each of the image  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(trainX[i], cmap=plt.cm.binary)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('LABEL for each of the image ', trainY[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4TbJGeSOIU4"
   },
   "source": [
    "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac06XZZTOIU6"
   },
   "outputs": [],
   "source": [
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#Comile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hQpLv3aOIU_"
   },
   "source": [
    "### Execute the model using model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "O59C_-IgOIVB",
    "outputId": "ad861a97-b42a-45f6-dd7b-0f78c02de8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 2.8133 - acc: 0.1392 - val_loss: 19.3164 - val_acc: 0.1386\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 2.5631 - acc: 0.1663 - val_loss: 11.8917 - val_acc: 0.1621\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 2.3530 - acc: 0.1980 - val_loss: 8.5393 - val_acc: 0.1881\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 2.1767 - acc: 0.2342 - val_loss: 6.6137 - val_acc: 0.2208\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 2.0286 - acc: 0.2732 - val_loss: 5.3892 - val_acc: 0.2517\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.9036 - acc: 0.3130 - val_loss: 4.5599 - val_acc: 0.2778\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.7978 - acc: 0.3505 - val_loss: 3.9716 - val_acc: 0.3097\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.7077 - acc: 0.3850 - val_loss: 3.5387 - val_acc: 0.3343\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.6304 - acc: 0.4166 - val_loss: 3.2094 - val_acc: 0.3562\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.5638 - acc: 0.4448 - val_loss: 2.9515 - val_acc: 0.3768\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.5059 - acc: 0.4685 - val_loss: 2.7443 - val_acc: 0.3998\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.4552 - acc: 0.4895 - val_loss: 2.5740 - val_acc: 0.4182\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.4105 - acc: 0.5084 - val_loss: 2.4315 - val_acc: 0.4358\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.3708 - acc: 0.5255 - val_loss: 2.3103 - val_acc: 0.4513\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.3354 - acc: 0.5405 - val_loss: 2.2057 - val_acc: 0.4660\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.3035 - acc: 0.5537 - val_loss: 2.1145 - val_acc: 0.4796\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.2747 - acc: 0.5651 - val_loss: 2.0340 - val_acc: 0.4920\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.2485 - acc: 0.5762 - val_loss: 1.9624 - val_acc: 0.5042\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.2246 - acc: 0.5859 - val_loss: 1.8982 - val_acc: 0.5162\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.2026 - acc: 0.5942 - val_loss: 1.8402 - val_acc: 0.5283\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.1824 - acc: 0.6020 - val_loss: 1.7875 - val_acc: 0.5378\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.1636 - acc: 0.6088 - val_loss: 1.7393 - val_acc: 0.5473\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.1462 - acc: 0.6147 - val_loss: 1.6951 - val_acc: 0.5552\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.1300 - acc: 0.6205 - val_loss: 1.6544 - val_acc: 0.5635\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.1148 - acc: 0.6257 - val_loss: 1.6167 - val_acc: 0.5715\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.1005 - acc: 0.6306 - val_loss: 1.5816 - val_acc: 0.5786\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.0871 - acc: 0.6352 - val_loss: 1.5490 - val_acc: 0.5856\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.0745 - acc: 0.6397 - val_loss: 1.5185 - val_acc: 0.5932\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.0626 - acc: 0.6438 - val_loss: 1.4899 - val_acc: 0.5984\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.0513 - acc: 0.6477 - val_loss: 1.4630 - val_acc: 0.6032\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.0406 - acc: 0.6517 - val_loss: 1.4376 - val_acc: 0.6081\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.0304 - acc: 0.6553 - val_loss: 1.4137 - val_acc: 0.6128\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.0207 - acc: 0.6583 - val_loss: 1.3911 - val_acc: 0.6168\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 1.0114 - acc: 0.6614 - val_loss: 1.3697 - val_acc: 0.6198\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 1.0026 - acc: 0.6641 - val_loss: 1.3493 - val_acc: 0.6244\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.9941 - acc: 0.6668 - val_loss: 1.3299 - val_acc: 0.6276\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.9860 - acc: 0.6694 - val_loss: 1.3114 - val_acc: 0.6309\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.9783 - acc: 0.6720 - val_loss: 1.2938 - val_acc: 0.6344\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.9708 - acc: 0.6750 - val_loss: 1.2770 - val_acc: 0.6382\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.9636 - acc: 0.6773 - val_loss: 1.2609 - val_acc: 0.6417\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.9567 - acc: 0.6799 - val_loss: 1.2454 - val_acc: 0.6460\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.9501 - acc: 0.6820 - val_loss: 1.2306 - val_acc: 0.6488\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.9437 - acc: 0.6841 - val_loss: 1.2164 - val_acc: 0.6513\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.9375 - acc: 0.6863 - val_loss: 1.2027 - val_acc: 0.6537\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.9315 - acc: 0.6879 - val_loss: 1.1895 - val_acc: 0.6552\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.9257 - acc: 0.6898 - val_loss: 1.1768 - val_acc: 0.6574\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.9202 - acc: 0.6912 - val_loss: 1.1646 - val_acc: 0.6593\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.9148 - acc: 0.6928 - val_loss: 1.1528 - val_acc: 0.6612\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.9095 - acc: 0.6946 - val_loss: 1.1414 - val_acc: 0.6630\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.9044 - acc: 0.6964 - val_loss: 1.1304 - val_acc: 0.6656\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8995 - acc: 0.6982 - val_loss: 1.1198 - val_acc: 0.6665\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8947 - acc: 0.6999 - val_loss: 1.1095 - val_acc: 0.6683\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8901 - acc: 0.7014 - val_loss: 1.0995 - val_acc: 0.6701\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8855 - acc: 0.7029 - val_loss: 1.0899 - val_acc: 0.6724\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8811 - acc: 0.7042 - val_loss: 1.0805 - val_acc: 0.6739\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8769 - acc: 0.7054 - val_loss: 1.0714 - val_acc: 0.6752\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8727 - acc: 0.7065 - val_loss: 1.0626 - val_acc: 0.6765\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8686 - acc: 0.7078 - val_loss: 1.0541 - val_acc: 0.6786\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8647 - acc: 0.7092 - val_loss: 1.0458 - val_acc: 0.6801\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8608 - acc: 0.7106 - val_loss: 1.0377 - val_acc: 0.6816\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8571 - acc: 0.7116 - val_loss: 1.0298 - val_acc: 0.6830\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8534 - acc: 0.7134 - val_loss: 1.0222 - val_acc: 0.6850\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8498 - acc: 0.7145 - val_loss: 1.0148 - val_acc: 0.6857\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8463 - acc: 0.7155 - val_loss: 1.0076 - val_acc: 0.6872\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8429 - acc: 0.7168 - val_loss: 1.0005 - val_acc: 0.6884\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8395 - acc: 0.7177 - val_loss: 0.9937 - val_acc: 0.6900\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8363 - acc: 0.7187 - val_loss: 0.9870 - val_acc: 0.6915\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8331 - acc: 0.7197 - val_loss: 0.9805 - val_acc: 0.6924\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8299 - acc: 0.7206 - val_loss: 0.9742 - val_acc: 0.6934\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8269 - acc: 0.7216 - val_loss: 0.9680 - val_acc: 0.6952\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8239 - acc: 0.7226 - val_loss: 0.9620 - val_acc: 0.6964\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8209 - acc: 0.7236 - val_loss: 0.9561 - val_acc: 0.6973\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8181 - acc: 0.7243 - val_loss: 0.9504 - val_acc: 0.6988\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8152 - acc: 0.7253 - val_loss: 0.9448 - val_acc: 0.7001\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8125 - acc: 0.7263 - val_loss: 0.9393 - val_acc: 0.7017\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8097 - acc: 0.7272 - val_loss: 0.9340 - val_acc: 0.7030\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.8071 - acc: 0.7279 - val_loss: 0.9288 - val_acc: 0.7035\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8045 - acc: 0.7288 - val_loss: 0.9237 - val_acc: 0.7048\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.8019 - acc: 0.7297 - val_loss: 0.9187 - val_acc: 0.7058\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7994 - acc: 0.7305 - val_loss: 0.9139 - val_acc: 0.7066\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7969 - acc: 0.7310 - val_loss: 0.9091 - val_acc: 0.7073\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7945 - acc: 0.7318 - val_loss: 0.9045 - val_acc: 0.7080\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7921 - acc: 0.7326 - val_loss: 0.9000 - val_acc: 0.7092\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7898 - acc: 0.7332 - val_loss: 0.8955 - val_acc: 0.7100\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7875 - acc: 0.7339 - val_loss: 0.8912 - val_acc: 0.7109\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7852 - acc: 0.7346 - val_loss: 0.8869 - val_acc: 0.7115\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7830 - acc: 0.7355 - val_loss: 0.8828 - val_acc: 0.7129\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7808 - acc: 0.7362 - val_loss: 0.8787 - val_acc: 0.7135\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7786 - acc: 0.7369 - val_loss: 0.8747 - val_acc: 0.7143\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7765 - acc: 0.7374 - val_loss: 0.8708 - val_acc: 0.7157\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7744 - acc: 0.7382 - val_loss: 0.8670 - val_acc: 0.7169\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7724 - acc: 0.7390 - val_loss: 0.8633 - val_acc: 0.7174\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7703 - acc: 0.7397 - val_loss: 0.8596 - val_acc: 0.7186\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7684 - acc: 0.7404 - val_loss: 0.8560 - val_acc: 0.7194\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7664 - acc: 0.7414 - val_loss: 0.8525 - val_acc: 0.7198\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7645 - acc: 0.7420 - val_loss: 0.8491 - val_acc: 0.7203\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7626 - acc: 0.7427 - val_loss: 0.8457 - val_acc: 0.7212\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7607 - acc: 0.7432 - val_loss: 0.8424 - val_acc: 0.7222\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7588 - acc: 0.7437 - val_loss: 0.8392 - val_acc: 0.7231\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7570 - acc: 0.7441 - val_loss: 0.8360 - val_acc: 0.7242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f58f3300668>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, \n",
    "          validation_data=(testX, testY), \n",
    "          epochs=100,\n",
    "          batch_size=trainX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6Euk01q8Jhf"
   },
   "outputs": [],
   "source": [
    "#model Accuracy Improved from 13% to 72%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdzDtGwDOIVF"
   },
   "source": [
    "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kndfpdidOIVI"
   },
   "outputs": [],
   "source": [
    "##Batch Normalization used in the above step only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwk3T5LJOIVN"
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "JNLR8tcBOIVP",
    "outputId": "f29cfd87-368e-4415-cdc7-606fb45ef1ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 10,986\n",
      "Trainable params: 9,418\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Py-KwkmjOIVU"
   },
   "source": [
    "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yLXUE9jWOIVV",
    "outputId": "e3bfd2f3-abc6-4972-fc90-6c1e5b3247ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7552 - acc: 0.7449 - val_loss: 0.8329 - val_acc: 0.7249\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7535 - acc: 0.7455 - val_loss: 0.8298 - val_acc: 0.7253\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7517 - acc: 0.7461 - val_loss: 0.8268 - val_acc: 0.7262\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7500 - acc: 0.7467 - val_loss: 0.8239 - val_acc: 0.7275\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7483 - acc: 0.7472 - val_loss: 0.8210 - val_acc: 0.7283\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7466 - acc: 0.7477 - val_loss: 0.8182 - val_acc: 0.7287\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7450 - acc: 0.7483 - val_loss: 0.8154 - val_acc: 0.7292\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7434 - acc: 0.7487 - val_loss: 0.8127 - val_acc: 0.7298\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7417 - acc: 0.7490 - val_loss: 0.8100 - val_acc: 0.7302\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7402 - acc: 0.7495 - val_loss: 0.8074 - val_acc: 0.7308\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7386 - acc: 0.7499 - val_loss: 0.8048 - val_acc: 0.7316\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7370 - acc: 0.7505 - val_loss: 0.8023 - val_acc: 0.7325\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7355 - acc: 0.7509 - val_loss: 0.7998 - val_acc: 0.7331\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7340 - acc: 0.7514 - val_loss: 0.7973 - val_acc: 0.7336\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7325 - acc: 0.7521 - val_loss: 0.7949 - val_acc: 0.7336\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7311 - acc: 0.7525 - val_loss: 0.7926 - val_acc: 0.7343\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7296 - acc: 0.7530 - val_loss: 0.7903 - val_acc: 0.7351\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7282 - acc: 0.7536 - val_loss: 0.7880 - val_acc: 0.7356\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7268 - acc: 0.7539 - val_loss: 0.7858 - val_acc: 0.7360\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7253 - acc: 0.7546 - val_loss: 0.7836 - val_acc: 0.7361\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7240 - acc: 0.7549 - val_loss: 0.7814 - val_acc: 0.7366\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7226 - acc: 0.7554 - val_loss: 0.7793 - val_acc: 0.7368\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7212 - acc: 0.7560 - val_loss: 0.7772 - val_acc: 0.7379\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7199 - acc: 0.7565 - val_loss: 0.7752 - val_acc: 0.7384\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7186 - acc: 0.7571 - val_loss: 0.7732 - val_acc: 0.7383\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7173 - acc: 0.7574 - val_loss: 0.7712 - val_acc: 0.7389\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7160 - acc: 0.7580 - val_loss: 0.7692 - val_acc: 0.7396\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7147 - acc: 0.7586 - val_loss: 0.7673 - val_acc: 0.7400\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7134 - acc: 0.7590 - val_loss: 0.7654 - val_acc: 0.7410\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7122 - acc: 0.7593 - val_loss: 0.7636 - val_acc: 0.7414\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7110 - acc: 0.7598 - val_loss: 0.7617 - val_acc: 0.7415\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7097 - acc: 0.7602 - val_loss: 0.7599 - val_acc: 0.7416\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7085 - acc: 0.7605 - val_loss: 0.7582 - val_acc: 0.7419\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7073 - acc: 0.7608 - val_loss: 0.7564 - val_acc: 0.7420\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7061 - acc: 0.7610 - val_loss: 0.7547 - val_acc: 0.7424\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7050 - acc: 0.7614 - val_loss: 0.7530 - val_acc: 0.7424\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7038 - acc: 0.7617 - val_loss: 0.7514 - val_acc: 0.7428\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7027 - acc: 0.7620 - val_loss: 0.7497 - val_acc: 0.7435\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.7015 - acc: 0.7624 - val_loss: 0.7481 - val_acc: 0.7443\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.7004 - acc: 0.7629 - val_loss: 0.7465 - val_acc: 0.7446\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6993 - acc: 0.7633 - val_loss: 0.7450 - val_acc: 0.7450\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6982 - acc: 0.7636 - val_loss: 0.7434 - val_acc: 0.7455\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6971 - acc: 0.7639 - val_loss: 0.7419 - val_acc: 0.7458\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6960 - acc: 0.7642 - val_loss: 0.7404 - val_acc: 0.7458\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6949 - acc: 0.7646 - val_loss: 0.7389 - val_acc: 0.7467\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6939 - acc: 0.7652 - val_loss: 0.7375 - val_acc: 0.7471\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6928 - acc: 0.7655 - val_loss: 0.7361 - val_acc: 0.7474\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6918 - acc: 0.7659 - val_loss: 0.7347 - val_acc: 0.7473\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6908 - acc: 0.7660 - val_loss: 0.7333 - val_acc: 0.7475\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6897 - acc: 0.7664 - val_loss: 0.7319 - val_acc: 0.7481\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6887 - acc: 0.7666 - val_loss: 0.7305 - val_acc: 0.7486\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6877 - acc: 0.7669 - val_loss: 0.7292 - val_acc: 0.7491\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6867 - acc: 0.7672 - val_loss: 0.7279 - val_acc: 0.7497\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6857 - acc: 0.7675 - val_loss: 0.7266 - val_acc: 0.7502\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6848 - acc: 0.7678 - val_loss: 0.7253 - val_acc: 0.7507\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6838 - acc: 0.7681 - val_loss: 0.7241 - val_acc: 0.7513\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6828 - acc: 0.7685 - val_loss: 0.7228 - val_acc: 0.7521\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6819 - acc: 0.7688 - val_loss: 0.7216 - val_acc: 0.7525\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6810 - acc: 0.7690 - val_loss: 0.7204 - val_acc: 0.7527\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6800 - acc: 0.7692 - val_loss: 0.7192 - val_acc: 0.7527\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6791 - acc: 0.7695 - val_loss: 0.7180 - val_acc: 0.7532\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6782 - acc: 0.7699 - val_loss: 0.7169 - val_acc: 0.7538\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6773 - acc: 0.7702 - val_loss: 0.7157 - val_acc: 0.7540\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6764 - acc: 0.7704 - val_loss: 0.7146 - val_acc: 0.7547\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6755 - acc: 0.7707 - val_loss: 0.7135 - val_acc: 0.7552\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6746 - acc: 0.7710 - val_loss: 0.7124 - val_acc: 0.7553\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6737 - acc: 0.7712 - val_loss: 0.7113 - val_acc: 0.7551\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6728 - acc: 0.7715 - val_loss: 0.7102 - val_acc: 0.7552\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6720 - acc: 0.7718 - val_loss: 0.7091 - val_acc: 0.7555\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6711 - acc: 0.7721 - val_loss: 0.7081 - val_acc: 0.7554\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6703 - acc: 0.7722 - val_loss: 0.7070 - val_acc: 0.7557\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6694 - acc: 0.7725 - val_loss: 0.7060 - val_acc: 0.7561\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6686 - acc: 0.7728 - val_loss: 0.7050 - val_acc: 0.7564\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6678 - acc: 0.7730 - val_loss: 0.7040 - val_acc: 0.7565\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6670 - acc: 0.7734 - val_loss: 0.7030 - val_acc: 0.7568\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6661 - acc: 0.7735 - val_loss: 0.7020 - val_acc: 0.7572\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6653 - acc: 0.7739 - val_loss: 0.7011 - val_acc: 0.7576\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6645 - acc: 0.7740 - val_loss: 0.7001 - val_acc: 0.7583\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6637 - acc: 0.7743 - val_loss: 0.6992 - val_acc: 0.7587\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6629 - acc: 0.7746 - val_loss: 0.6982 - val_acc: 0.7593\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6622 - acc: 0.7748 - val_loss: 0.6973 - val_acc: 0.7592\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6614 - acc: 0.7751 - val_loss: 0.6964 - val_acc: 0.7599\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6606 - acc: 0.7754 - val_loss: 0.6955 - val_acc: 0.7603\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6599 - acc: 0.7757 - val_loss: 0.6946 - val_acc: 0.7607\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6591 - acc: 0.7760 - val_loss: 0.6937 - val_acc: 0.7612\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6583 - acc: 0.7762 - val_loss: 0.6929 - val_acc: 0.7614\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6576 - acc: 0.7764 - val_loss: 0.6920 - val_acc: 0.7615\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6569 - acc: 0.7766 - val_loss: 0.6911 - val_acc: 0.7618\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.6561 - acc: 0.7769 - val_loss: 0.6903 - val_acc: 0.7623\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6554 - acc: 0.7772 - val_loss: 0.6895 - val_acc: 0.7627\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6547 - acc: 0.7774 - val_loss: 0.6886 - val_acc: 0.7630\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6539 - acc: 0.7776 - val_loss: 0.6878 - val_acc: 0.7634\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6532 - acc: 0.7780 - val_loss: 0.6870 - val_acc: 0.7633\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6525 - acc: 0.7782 - val_loss: 0.6862 - val_acc: 0.7633\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6518 - acc: 0.7786 - val_loss: 0.6854 - val_acc: 0.7635\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6511 - acc: 0.7789 - val_loss: 0.6846 - val_acc: 0.7637\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6504 - acc: 0.7792 - val_loss: 0.6839 - val_acc: 0.7640\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6497 - acc: 0.7794 - val_loss: 0.6831 - val_acc: 0.7640\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6490 - acc: 0.7798 - val_loss: 0.6823 - val_acc: 0.7646\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.6484 - acc: 0.7801 - val_loss: 0.6816 - val_acc: 0.7648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f58f37b8d68>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.01)\n",
    "\n",
    "model.fit(trainX, trainY, \n",
    "          validation_data=(testX, testY), \n",
    "          epochs=100,\n",
    "          batch_size=trainX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJUqA5T4OIVc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9CSqKvpOIVk"
   },
   "source": [
    "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hr8CH6OO6dYr"
   },
   "outputs": [],
   "source": [
    "\n",
    "trainX = trainX.astype('float32')\n",
    "trainY = trainY.astype('float32')\n",
    "\n",
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Reshape data from 2D to 1D -> 28x28 to 784\n",
    "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(tf.keras.layers.BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGAad54JOIVm"
   },
   "outputs": [],
   "source": [
    "#Add 1st hidden layer\n",
    "model.add(tf.keras.layers.Dense(200, activation='sigmoid'))\n",
    "\n",
    "#Add 2nd hidden layer\n",
    "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "\n",
    "#Add OUTPUT layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQ7oIymROIVp"
   },
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "X-O-fFxnOIVt",
    "outputId": "c0ffe34e-566a-4b3f-c695-cb226229af51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 1.3973 - acc: 0.6335 - val_loss: 0.9081 - val_acc: 0.7334\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.7843 - acc: 0.7462 - val_loss: 0.6811 - val_acc: 0.7616\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.6441 - acc: 0.7739 - val_loss: 0.5988 - val_acc: 0.7839\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.5781 - acc: 0.7938 - val_loss: 0.5508 - val_acc: 0.7996\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.5355 - acc: 0.8098 - val_loss: 0.5176 - val_acc: 0.8116\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.5094 - acc: 0.8195 - val_loss: 0.4967 - val_acc: 0.8173\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.4890 - acc: 0.8267 - val_loss: 0.4809 - val_acc: 0.8247\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.4726 - acc: 0.8336 - val_loss: 0.4693 - val_acc: 0.8295\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.4613 - acc: 0.8356 - val_loss: 0.4601 - val_acc: 0.8337\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.4505 - acc: 0.8396 - val_loss: 0.4506 - val_acc: 0.8349\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4414 - acc: 0.8417 - val_loss: 0.4436 - val_acc: 0.8377\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4335 - acc: 0.8463 - val_loss: 0.4376 - val_acc: 0.8407\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.4256 - acc: 0.8485 - val_loss: 0.4335 - val_acc: 0.8433\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.4188 - acc: 0.8510 - val_loss: 0.4273 - val_acc: 0.8425\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.4155 - acc: 0.8521 - val_loss: 0.4224 - val_acc: 0.8453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f58eb4f4c18>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX,trainY,          \n",
    "          validation_data=(testX,testY),\n",
    "          epochs=15,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiP7IL52OIVw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr2YsZV0OIV0"
   },
   "source": [
    "## Review model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4ojW6-oOIV2"
   },
   "outputs": [],
   "source": [
    "##As Per the Model the Accuracy reached  from 73% to  84% and Loss reduced from 0.90 to 0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "pjSBb5bY7w7N",
    "outputId": "2f9d20e1-d8d7-47df-ea21-014430dc6c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 181,246\n",
      "Trainable params: 179,678\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfFGmbZLOIV5"
   },
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIkbMEN5OIV7"
   },
   "outputs": [],
   "source": [
    "##Saving the Model\n",
    "model.save('mnist_lc_v2.h2')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R6_ExternalLab_AIML.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
